<!doctype html><html><head><meta charset="utf-8">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js">
<link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/texmath.css">
<link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/vscode-texmath.css">

</head><body>
<h1 id="nlp-concepts-and-papers" data-line="0" class="code-line">NLP Concepts and Papers</h1>
<!-- TOC -->
<ul>
<li data-line="3" class="code-line"><a href="#nlp-concepts-and-papers" data-href="#nlp-concepts-and-papers">NLP Concepts and Papers</a>
<ul>
<li data-line="4" class="code-line"><a href="#transformers" data-href="#transformers">Transformers</a></li>
<li data-line="5" class="code-line"><a href="#transformer-xl" data-href="#transformer-xl">Transformer-XL</a></li>
<li data-line="6" class="code-line"><a href="#longformers" data-href="#longformers">LongFormers</a></li>
<li data-line="7" class="code-line"><a href="#bert" data-href="#bert">BERT</a></li>
<li data-line="8" class="code-line"><a href="#trade-for-tod-systems" data-href="#trade-for-tod-systems">TRADE for TOD Systems</a></li>
<li data-line="9" class="code-line"><a href="#unlikelihood-loss" data-href="#unlikelihood-loss">Unlikelihood Loss</a></li>
<li data-line="10" class="code-line"><a href="#truncated-log-loss" data-href="#truncated-log-loss">Truncated Log Loss</a></li>
<li data-line="11" class="code-line"><a href="#other-papers" data-href="#other-papers">Other Papers</a></li>
<li data-line="12" class="code-line"><a href="#template" data-href="#template">Template</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h2 id="transformers" data-line="15" class="code-line">Transformers</h2>
<p data-line="16" class="code-line"><strong>Resources</strong>: <a href="https://arxiv.org/pdf/1706.03762.pdf" data-href="https://arxiv.org/pdf/1706.03762.pdf">paper</a>, <a href="http://jalammar.github.io/illustrated-transformer/" data-href="http://jalammar.github.io/illustrated-transformer/">illustrated guide</a>, <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" data-href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Annotated Transformer</a></p>
<p data-line="18" class="code-line"><strong>Top Advantages</strong></p>
<ul>
<li data-line="19" class="code-line">removes the recurrence</li>
<li data-line="20" class="code-line">easier to parallelise</li>
<li data-line="21" class="code-line">better long term dependencies</li>
<li data-line="22" class="code-line">resolves vanishing gradient problem with RNNs</li>
</ul>
<p data-line="24" class="code-line"><strong>New things</strong></p>
<ul>
<li data-line="25" class="code-line">Scaled-Dot Product Attention
<ul>
<li data-line="26" class="code-line">We have Query, Key and Values. Get a weighted sum of values depending on the relevance of keys with the query</li>
</ul>
</li>
<li data-line="27" class="code-line">Multi-Head Attention
<ul>
<li data-line="28" class="code-line">do scaled dot product but for multiple projections of Q,K and V in parallel and concat them</li>
<li data-line="29" class="code-line">allows calculations over different representation spaces</li>
</ul>
</li>
<li data-line="30" class="code-line">position encoding method</li>
</ul>
<p data-line="32" class="code-line"><strong>Model Summary</strong></p>
<ul>
<li data-line="33" class="code-line">Uses a new position encoding technique to preserve order - uses absolute positions</li>
<li data-line="34" class="code-line">Use a few layers of multi-head attention on input over input (self-attention with ) in encoder.</li>
<li data-line="35" class="code-line">Use this in each 'cell' of decoder.</li>
<li data-line="36" class="code-line">each cell of decoder has Self-attention in output (masked to avoid taking information from words ahead in the sentence), then attention over encoding with self-attention output.
<img src="images/NLP-transformer1.png" alt="" class="loading" id="image-hash-00b6e1d9a47f96e8bbd61fa170e3646bf84e3b24629d6a0e61b719b1c72d128a">
<img src="images/NLP-transformer2.png" alt="" class="loading" id="image-hash-dc0381260c338b78b802ef349a32f57ccecbc8b3d2c5b144e71d3a8660126c18"></li>
</ul>
<p data-line="40" class="code-line"><strong>Notable Insights</strong></p>
<ul>
<li data-line="41" class="code-line">self-attention
<ul>
<li data-line="42" class="code-line">computationally cheaper than RNN - optimized Matrix Multiplication</li>
<li data-line="43" class="code-line">easier to propagate from output to input (Maximum path length is O(1))</li>
<li data-line="44" class="code-line">Self-attention allows the model to look at the other words in the input sequence to get a better understanding of a certain word in the sequence. Eg. 'Dog bit it's tail'. 'It' is related to 'Dog'. More <a href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/" data-href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/">here</a></li>
</ul>
</li>
</ul>
<p data-line="46" class="code-line"><strong>Limitations</strong></p>
<ul>
<li data-line="47" class="code-line">Can handle only fixed length inputs. So need to split input into chunks</li>
<li data-line="48" class="code-line">chunked data -&gt; limited range of dependencies</li>
<li data-line="49" class="code-line">if you chunk sentences to feed into transformers, then you have the problem of <strong>context fragmentation</strong> because a split sentence looses context present in other chunks</li>
<li data-line="50" class="code-line">computation cost scales quadratically with sequence length</li>
</ul>
<p data-line="52" class="code-line"><strong>Questions</strong></p>
<ul>
<li data-line="53" class="code-line">what's the logic behind self-attention, how exactly is it helping? The logic presented only explains the benefit of the attention mechanism which holds true for using the multi-head attention without self attention. What exactly is the intuition here?</li>
<li data-line="54" class="code-line">why is masking necessary in transformers? - To make sure output at i depends only on outputs before position i. But WHY is this important? Why not just mask itself?</li>
<li data-line="55" class="code-line">Can someone explain their position encoding scheme to me?</li>
<li data-line="56" class="code-line">(Solved) How exactly does it avoid recurrence? Because decoder states depend on the encoder finishing it's job -&gt; That's only Layering. If one layer, then it's just two steps of sequential attentions.</li>
</ul>
<h2 id="transformer-xl" data-line="58" class="code-line">Transformer-XL</h2>
<p data-line="59" class="code-line"><strong>Resources</strong>: <a href="" data-href="">paper</a></p>
<p data-line="61" class="code-line"><strong>Top Advantanges</strong></p>
<ul>
<li data-line="62" class="code-line">Captures longer-term dependencies</li>
<li data-line="63" class="code-line">resolves context fragmentation to some extent</li>
<li data-line="64" class="code-line">&quot;Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.&quot;</li>
</ul>
<p data-line="66" class="code-line"><strong>New things</strong></p>
<ul>
<li data-line="67" class="code-line">Recurrence mechanism to transformers
<ul>
<li data-line="68" class="code-line">Instead of computing the hidden states from scratch for each new segment, concat/reuse the hidden states obtained in previous segments.</li>
<li data-line="69" class="code-line">The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments.</li>
<li data-line="70" class="code-line">Thus, information can be propogated -&gt; modeling very long-term dependency becomes possible and solves context fragmentation (3.2 in paper for formal definition)</li>
<li data-line="71" class="code-line">Note: Gradient remains within one segment. Only the segment encoding goes to next segment to provide context</li>
<li data-line="72" class="code-line">can concat multiple previous segments to increase range of the model (depends on available memory)</li>
</ul>
</li>
<li data-line="73" class="code-line">New Relative position encoding
<ul>
<li data-line="74" class="code-line">&quot;avoids temporal confusion&quot;, &quot;generalizes to attention lengths longer than the one observed during training&quot;</li>
<li data-line="75" class="code-line">instead of using an absolute position encoding, use the relative distance between the positions. Ex. when calculating attention score between <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></eq> and <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">x_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></eq>, use <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>âˆ’</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{i-j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">âˆ’</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></eq> (sinusoid encoding matrix) for position instead of the absolute one in original transformers</li>
</ul>
</li>
</ul>
<p data-line="77" class="code-line"><strong>Model Summary</strong></p>
<ul>
<li data-line="78" class="code-line">In any layer of the encoder, concat the prev-layer output for previous segment to the key and value, then use the layer.</li>
<li data-line="79" class="code-line">This combined with relative position encoding enables model to understand the position of the context words relative to current words -&gt; longer dependency
<img src="images/NLP-transformerXL.png" alt="" class="loading" id="image-hash-514badd9c41b719ce9de5a6a235a65785638f153f5305512b2a78e6206ee5961"></li>
</ul>
<p data-line="82" class="code-line"><strong>Notable Insights</strong></p>
<p data-line="84" class="code-line"><strong>Misc things</strong>
: It is for the LM task.</p>
<p data-line="87" class="code-line"><strong>limitations</strong></p>
<ul>
<li data-line="88" class="code-line">Not pretrained</li>
<li data-line="89" class="code-line">computation cost still scales quadratically with sequence length</li>
</ul>
<p data-line="91" class="code-line"><strong>Questions</strong></p>
<ul>
<li data-line="92" class="code-line">What is sinusoid encoding matrix</li>
<li data-line="93" class="code-line">Understand the position encoding used</li>
</ul>
<h2 id="longformers" data-line="95" class="code-line">LongFormers</h2>
<p data-line="97" class="code-line"><strong>Resources:</strong>
<a href="https://arxiv.org/pdf/2004.05150.pdf" data-href="https://arxiv.org/pdf/2004.05150.pdf">Paper</a></p>
<p data-line="100" class="code-line"><strong>Top Advantages</strong></p>
<ul>
<li data-line="101" class="code-line">Pretrained on multiple tasks</li>
<li data-line="102" class="code-line">computation scales linearly with input seq length as compared to quadratic in transformer</li>
<li data-line="103" class="code-line">This attention pattern can be plugged into any pretrained transformer model without the need to change the model architecture: (Eg. they used RoBERTa checkpoint for their model. section 5)</li>
<li data-line="104" class="code-line">Was able to run on sequence lengths of 32,256 compared to 512 in BERT</li>
</ul>
<p data-line="106" class="code-line"><strong>New Things</strong></p>
<p data-line="108" class="code-line">New Attention mechanism. Combo of two</p>
<ul>
<li data-line="109" class="code-line">Local-context self attention: learn context representations</li>
<li data-line="110" class="code-line">global attention: &quot;end task motivated&quot;, builds full sentence representations for task</li>
<li data-line="111" class="code-line">New custom CUDA kernel TVM for performing the banded matrix multiplications which are not currently supported by DL libraries</li>
</ul>
<p data-line="113" class="code-line">Attention Patterns</p>
<ul>
<li data-line="114" class="code-line">Sliding Window
<ul>
<li data-line="115" class="code-line">Attend over a window w around the token</li>
<li data-line="116" class="code-line"><eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo>Ã—</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n\times w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></eq></li>
<li data-line="117" class="code-line">as you stack <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span></eq> layers, the &quot;receptive field&quot; increases (like in CNN) to <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mo>Ã—</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">l\times w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span></eq></li>
<li data-line="118" class="code-line"><eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span></eq> becomes hyperparameter</li>
</ul>
</li>
<li data-line="119" class="code-line">Dilated Window
<ul>
<li data-line="120" class="code-line">use dilated window like in CNN kernels</li>
<li data-line="121" class="code-line">increases receptive field without increasing computation</li>
<li data-line="122" class="code-line">receptive field becomes <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mo>Ã—</mo><mi>d</mi><mo>Ã—</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">l\times d\times w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span></eq>, scales to thousands of tokens</li>
<li data-line="123" class="code-line">Use different values of <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span></eq> for different heads in Multi-Head Attention: small value for local context and large value for longer context</li>
</ul>
</li>
<li data-line="124" class="code-line">Global Attention
<ul>
<li data-line="125" class="code-line">For some selected input locations, attend to all locations and all locations attend to that token (symmetric)</li>
<li data-line="126" class="code-line">Task specific. Ex, for QA, global for question tokens</li>
<li data-line="127" class="code-line">only for a select tokens <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi><mo>&lt;</mo><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k&lt;&lt;n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span></span><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span></eq>, so complexity still remains <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></eq></li>
</ul>
</li>
</ul>
<p data-line="129" class="code-line"><strong>Model Summary</strong></p>
<ul>
<li data-line="130" class="code-line">uses two sets of projects of Q,K,V. One for sliding window and other for global attention - flexible use depending on task</li>
<li data-line="131" class="code-line">use different dilations in different projections for sliding window
<img src="images/NLP-Longtransformer.png" alt="" class="loading" id="image-hash-2b964d4f33f2872ad6a1760d23e23189e80293d5f11ae07558060ac8a3499106"></li>
<li data-line="133" class="code-line">LM task: increase window size for upper layers. Dilation only in a small number of heads in upper layers</li>
</ul>
<p data-line="135" class="code-line"><strong>Notable Insights</strong></p>
<ul>
<li data-line="136" class="code-line">Attention matrix in transformers
<ul>
<li data-line="137" class="code-line">Left-to-right: process chunks from ltr. Good for autoregressive modelling tasks like LM. Looses out when model can benefit from bidirectional context</li>
<li data-line="138" class="code-line">Sparse: Don't compute the full <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></eq> attention matrix. ex. Dilated sliding window by <a href="https://openai.com/blog/block-sparse-gpu-kernels/" data-href="https://openai.com/blog/block-sparse-gpu-kernels/">BlockSparse</a> in <a href="https://arxiv.org/pdf/1904.10509" data-href="https://arxiv.org/pdf/1904.10509">sparse transformer</a></li>
<li data-line="139" class="code-line">can plug in the long version into other transformer models. <a href="https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb" data-href="https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb">Script</a></li>
</ul>
</li>
</ul>
<p data-line="141" class="code-line"><strong>Misc things</strong>
: For LM and long document modelling, paper used character level model.</p>
<p data-line="144" class="code-line"><strong>limitations</strong></p>
<p data-line="146" class="code-line"><strong>Questions</strong></p>
<ul>
<li data-line="147" class="code-line">What is <a href="https://openai.com/blog/block-sparse-gpu-kernels/" data-href="https://openai.com/blog/block-sparse-gpu-kernels/">BlockSparse</a> and <a href="https://arxiv.org/pdf/1904.10509" data-href="https://arxiv.org/pdf/1904.10509">sparse transformer</a></li>
<li data-line="148" class="code-line">Training strategy of this model for LM (section 4.2)</li>
<li data-line="149" class="code-line">TODO:Read up how it is pretrained for different tasks</li>
</ul>
<h2 id="bert" data-line="151" class="code-line">BERT</h2>
<p data-line="153" class="code-line"><strong>Resources</strong>
: <a href="https://arxiv.org/pdf/1810.04805.pdf" data-href="https://arxiv.org/pdf/1810.04805.pdf">Paper</a>, <a href="https://stats.stackexchange.com/questions/193082/what-is-pre-training-a-neural-network" data-href="https://stats.stackexchange.com/questions/193082/what-is-pre-training-a-neural-network">Pretrained Model</a>, <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" data-href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">blog</a></p>
<p data-line="156" class="code-line"><strong>Top Advantages</strong></p>
<ul>
<li data-line="157" class="code-line">Strong Pretrained model (for both fine-tuning and feature based approach)</li>
<li data-line="158" class="code-line">Bidirectional</li>
<li data-line="159" class="code-line">single pre-trained model is SOTA for a lot of tasks, beating tasks specific architectures - Unified architecture</li>
<li data-line="160" class="code-line">can have bidirectional cross attention between two sentences</li>
</ul>
<p data-line="162" class="code-line"><strong>New Things</strong></p>
<ul>
<li data-line="163" class="code-line">Deep bidirectional transformers: shows bidirectional pre-training gives better language representation</li>
<li data-line="164" class="code-line">MLM - Masked LM
<ul>
<li data-line="165" class="code-line">randomly mask some of the input tokens. Now try to predict these correct vocab id of these tokens using the surrounding context in pretraining</li>
<li data-line="166" class="code-line">mask 15% tokens and use softmax only on them. 80% of these are replaced with MASK token, 10% with random token, 10% with original token</li>
<li data-line="167" class="code-line">refer to the <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" data-href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">blog</a> - AppendixA for details</li>
</ul>
</li>
</ul>
<p data-line="169" class="code-line"><strong>Model Summary</strong></p>
<ul>
<li data-line="170" class="code-line">pretrain on unlabeled data, fine-tune on labeled data for downstream tasks</li>
<li data-line="171" class="code-line">Input (see fig2):
<ul>
<li data-line="172" class="code-line">seq of tokens - can be a single sentence, or a sentence pair (&lt;Q,A&gt;)</li>
<li data-line="173" class="code-line">begins with [CLS] token</li>
<li data-line="174" class="code-line">sparate sentences with [SEP] token and add a sentence embedding indicating Sentence A or Sentence B to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.</li>
<li data-line="175" class="code-line">Add positional embedding like in transformer
<img src="images/NLP-BERT2.png" alt="" class="loading" id="image-hash-ee2224b9b9f15e33cd14987bbb0297754380625b42d24353958807f4b83eafd4"></li>
</ul>
</li>
<li data-line="177" class="code-line">Training: Train using 2 tasks.
<ul>
<li data-line="178" class="code-line">MLM
<ul>
<li data-line="179" class="code-line">to get word representations</li>
<li data-line="180" class="code-line">softmax over masked words to calc loss</li>
</ul>
</li>
<li data-line="181" class="code-line">NSP
<ul>
<li data-line="182" class="code-line">use representation corresponding to [CLS] token</li>
<li data-line="183" class="code-line">use classificiation layer to predict if next sentence or not
<img src="images/NLP-BERT1.png" alt="" class="loading" id="image-hash-bb5455bcc8657312f210dd4be9cef6dabc4c6695c02bc71c3b2a3fb87a4a4bea"></li>
</ul>
</li>
</ul>
</li>
</ul>
<p data-line="186" class="code-line"><strong>Notable Insights</strong></p>
<p data-line="188" class="code-line">2 way of applying pre-training</p>
<ul>
<li data-line="189" class="code-line">feature based: used the pretrained representation as additional feature in the new architecture</li>
<li data-line="190" class="code-line">Fine-tuning: add some minimal task specific params and use the pre-trained params. Train on the new task and update ALL params.</li>
</ul>
<p data-line="192" class="code-line">MLM:</p>
<ul>
<li data-line="193" class="code-line">Models before BERT are restricted because they try to solve the unidirectional task of language modelling.</li>
<li data-line="194" class="code-line">new prediction goal for LM training. Allows to use bidirectional models</li>
<li data-line="195" class="code-line">without this, a bidirectional model will be able to indirectly see the word it is trying to predict</li>
</ul>
<p data-line="197" class="code-line"><strong>Misc things</strong></p>
<ul>
<li data-line="198" class="code-line">Output corresponding to token [CLS] to be used for classification tasks (eg. sentiment analysis), token representations to be used for token level tasks (eg. NER)</li>
<li data-line="199" class="code-line">Gives great performance when used as features directly - saves a lot of compute during finetuning</li>
<li data-line="200" class="code-line">hypothesizes that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task specific models can benefit from the larger, more expressive pre-trained representations even when
downstream task data is very small.</li>
</ul>
<p data-line="203" class="code-line"><strong>limitations</strong></p>
<ul>
<li data-line="204" class="code-line">unput token limit of 512</li>
<li data-line="205" class="code-line">converges slowly because loss is calculated only on the masked tokens</li>
</ul>
<p data-line="207" class="code-line"><strong>Questions</strong></p>
<ul>
<li data-line="208" class="code-line">Don't understand the problem it is solving</li>
<li data-line="209" class="code-line">How can it be used for LM?</li>
<li data-line="210" class="code-line">Masking - why is it done like it is done</li>
<li data-line="211" class="code-line">is the word-to-token embedding fine tuned/pre-trained/transferrable as well or not?</li>
<li data-line="212" class="code-line">TODO:Read up how it is fine tuned</li>
</ul>
<h2 id="trade-for-tod-systems" data-line="214" class="code-line">TRADE for TOD Systems</h2>
<p data-line="216" class="code-line"><strong>Resources</strong>
<a href="https://arxiv.org/pdf/1905.08743.pdf" data-href="https://arxiv.org/pdf/1905.08743.pdf">Paper</a></p>
<p data-line="219" class="code-line"><strong>Top Advantages</strong></p>
<ul>
<li data-line="220" class="code-line">generates dialogue states from utterances using a copy mechanism, facilitating knowledge transfer
when predicting (domain, slot, value) triplets
not encountered during training</li>
<li data-line="223" class="code-line">overcomes the problem of exhaustively listing all possible values of all slots (as required by previous models) - generates slot value directly instead of a probability of each value</li>
<li data-line="224" class="code-line">over comes lack of knowledge sharing between domains by sharing parameters for all domains. (eg. area slot can be shared between restaurant domain and hotel domain)</li>
<li data-line="225" class="code-line">zero shot learning with few examples for new domians withour forgetting old domains</li>
</ul>
<p data-line="227" class="code-line"><strong>New Things</strong></p>
<ul>
<li data-line="228" class="code-line">parameter sharing between domains</li>
<li data-line="229" class="code-line">directly generating slot value</li>
<li data-line="230" class="code-line">Utterance encoder
<ul>
<li data-line="231" class="code-line">dialog history utterances to sequence of fixed-length vectors (n utts to n vector representations)</li>
</ul>
</li>
<li data-line="232" class="code-line">context enhanced Slot gate:determine if any (domain,slot) value is mentioned. Outputs none, don't care and ptr</li>
<li data-line="233" class="code-line">state generator</li>
</ul>
<p data-line="235" class="code-line"><strong>Model Summary</strong></p>
<ul>
<li data-line="236" class="code-line">Utterance encoder to convert utterances to seq of fixed-length vectors <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">H_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></eq></li>
<li data-line="237" class="code-line">state generator decodes multiple output tokens for all (domain,slot) pairs independently</li>
<li data-line="238" class="code-line">state generator GRU
<ul>
<li data-line="239" class="code-line">first input is (domain,slot) encoding for each (domain,slot) pair</li>
<li data-line="240" class="code-line">at step k, takes in a word embedding to produce a hidden state</li>
<li data-line="241" class="code-line">hidden state is mapped to 2 distribution
<ul>
<li data-line="242" class="code-line">over voabulary - hidd * E</li>
<li data-line="243" class="code-line">over history - hidd * <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">H_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></eq></li>
</ul>
</li>
<li data-line="244" class="code-line">final prob is a weighted sum of these two (combines the two dist) - distribution over vocab</li>
<li data-line="245" class="code-line">weight is obtained using a MLP on hidden state, context and word embedding</li>
<li data-line="246" class="code-line">context vector is attened over <eq><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">H_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></eq> using the distribution over history</li>
</ul>
</li>
<li data-line="247" class="code-line">use Slot gate on context to figure out if the value is needed for this (domain, slot) pair</li>
<li data-line="248" class="code-line">optimization: cross entropy loss for slot gate and state generator</li>
</ul>
<p data-line="250" class="code-line"><strong>Notable Insights</strong></p>
<p data-line="252" class="code-line"><strong>Misc things</strong></p>
<ul>
<li data-line="253" class="code-line">achieves state-of-the-art joint goal accuracy of 48.62% for the five domains of MultiWOZ</li>
<li data-line="254" class="code-line">achieves 60.58% joint goal accuracy in one of
the zero-shot domains</li>
<li data-line="256" class="code-line">three types of copy mechanisms - index based, hard-gated, soft-gated</li>
</ul>
<p data-line="258" class="code-line"><strong>limitations</strong></p>
<p data-line="260" class="code-line"><strong>Questions</strong></p>
<ul>
<li data-line="261" class="code-line">read up on copy mechanisms!</li>
<li data-line="262" class="code-line">What is the word embedding at k step of state generator?</li>
<li data-line="263" class="code-line">read up on continual learning techniques - elastic weight consolidation (EWC) and gradient episodic memory (GEM)</li>
</ul>
<h2 id="unlikelihood-loss" data-line="265" class="code-line">Unlikelihood Loss</h2>
<p data-line="267" class="code-line"><strong>Resources</strong>
<a href="https://www.aclweb.org/anthology/2020.acl-main.428.pdf" data-href="https://www.aclweb.org/anthology/2020.acl-main.428.pdf">paper</a></p>
<p data-line="270" class="code-line"><strong>Top Advantages</strong></p>
<ul>
<li data-line="271" class="code-line">significant reduction in repitition of ngrams from context (copied from context) and within generated utterance</li>
</ul>
<p data-line="273" class="code-line"><strong>New Things</strong></p>
<ul>
<li data-line="274" class="code-line">add new term to loss function, which tells the model to
<ul>
<li data-line="275" class="code-line">reduce repitions - penalise is generated token is
<ul>
<li data-line="276" class="code-line">already in utterance</li>
<li data-line="277" class="code-line">already in context</li>
<li data-line="278" class="code-line">mix of both</li>
</ul>
</li>
<li data-line="279" class="code-line">pulls the model distribution of tokens closer to actual human distribution</li>
</ul>
</li>
</ul>
<p data-line="281" class="code-line"><strong>tags</strong>
loss,objective,unlikelihood</p>
<h2 id="truncated-log-loss" data-line="284" class="code-line">Truncated Log Loss</h2>
<p data-line="286" class="code-line"><strong>Resources</strong>
<a href="https://www.aclweb.org/anthology/2020.acl-main.66.pdf" data-href="https://www.aclweb.org/anthology/2020.acl-main.66.pdf">paper</a></p>
<p data-line="289" class="code-line"><strong>Top Advantages</strong></p>
<p data-line="291" class="code-line"><strong>New Things</strong></p>
<ul>
<li data-line="292" class="code-line">loss calculations</li>
<li data-line="293" class="code-line">rejection sampling</li>
</ul>
<p data-line="295" class="code-line"><strong>Model Summary</strong></p>
<ul>
<li data-line="296" class="code-line">hot start the model normally on log loss for some batches</li>
<li data-line="297" class="code-line">maintain an online estimate of quantile over batches</li>
<li data-line="298" class="code-line">remove examples with loss above the quantile</li>
</ul>
<p data-line="300" class="code-line"><strong>Notable Insights</strong></p>
<ul>
<li data-line="301" class="code-line">Log loss is very sensitive to outliers (check fig1 in paper)</li>
<li data-line="302" class="code-line">thus it picks up noise and other irrelevant abberations in data</li>
<li data-line="303" class="code-line">Better to optimize for <strong>Distinguisability</strong>
<ul>
<li data-line="304" class="code-line">indistinguishable generated text is better quality</li>
<li data-line="305" class="code-line">problem: challenging to do so</li>
</ul>
</li>
</ul>
<p data-line="307" class="code-line"><strong>Misc things</strong></p>
<p data-line="309" class="code-line"><strong>limitations</strong></p>
<p data-line="311" class="code-line"><strong>Questions</strong></p>
<ul>
<li data-line="312" class="code-line">Understand rejection sampling</li>
</ul>
<p data-line="314" class="code-line"><strong>tags</strong>
loss, objective, truncated, sensitivity</p>
<h2 id="other-papers" data-line="317" class="code-line">Other Papers</h2>
<ul>
<li data-line="318" class="code-line"><a href="https://arxiv.org/pdf/2003.07490.pdf" data-href="https://arxiv.org/pdf/2003.07490.pdf">TOD Survey(2020)</a>: Survey of Task Oriented Dialog Systems. Nicely summarises the problem and demarcates the latest models by the problems they solve.</li>
<li data-line="319" class="code-line"><a href="https://www.aclweb.org/anthology/P18-1136.pdf" data-href="https://www.aclweb.org/anthology/P18-1136.pdf">Mem2Seq</a>: SOTA model for TOD using KB and history. DID NOT UNDERSTAND the implementation at all.</li>
</ul>
<h2 id="template" data-line="322" class="code-line">Template</h2>
<p data-line="324" class="code-line"><strong>Resources</strong></p>
<p data-line="326" class="code-line"><strong>Top Advantages</strong></p>
<p data-line="328" class="code-line"><strong>New Things</strong></p>
<p data-line="330" class="code-line"><strong>Model Summary</strong></p>
<p data-line="332" class="code-line"><strong>Notable Insights</strong></p>
<p data-line="334" class="code-line"><strong>Misc things</strong></p>
<p data-line="336" class="code-line"><strong>limitations</strong></p>
<p data-line="338" class="code-line"><strong>Questions</strong></p>
<p data-line="340" class="code-line"><strong>tags</strong></p>

</body></html>